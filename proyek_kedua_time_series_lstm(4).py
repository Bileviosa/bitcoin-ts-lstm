# -*- coding: utf-8 -*-
"""Proyek Kedua - Time Series LSTM(4)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WAFnacqQjvsddp22P6_BXckwfgvCH-3X

load dataset
"""

import pandas as pd

url = './drive/MyDrive/Dataset Colabs/Time Series/bitcoin.csv'
data = pd.read_csv(url, parse_dates=['Date'], index_col='Date')
data

"""cek data set info """

data.info()

"""cek null data"""

data.isnull().sum()

"""cek pattern in weighted price each month"""

import matplotlib.pyplot as plt

time = data.index.values
test = data['Weighted Price'].values

plt.figure(figsize=(15,5))
plt.plot(time, test)

"""normalize weighted price using MinMaxScaler()"""

from sklearn.preprocessing import MinMaxScaler
values = data['Weighted Price'].values.reshape(-1,1)
values = values.astype('float32')
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)

"""split data into 80% train 20% test"""

train_size = int(len(scaled) * 0.8)
test_size = len(scaled) - train_size
train, test = scaled[0:train_size,:], scaled[train_size:len(scaled),:]
print(len(train), len(test))

"""create create_dataset() function"""

def create_dataset(dataset, look_back=1):
    dataX, dataY = [], []
    for i in range(len(dataset) - look_back):
        a = dataset[i:(i + look_back), 0]
        dataX.append(a)
        dataY.append(dataset[i + look_back, 0])
    print(len(dataY))
    return np.array(dataX), np.array(dataY)

"""generate dataset using create_dataset() function"""

import numpy as np
look_back = 1
trainX, trainY = create_dataset(train, look_back)
testX, testY = create_dataset(test, look_back)

"""reshape for model training"""

trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))

"""create myCallback() function to stop training when achieve mae < 0.01"""

import tensorflow as tf

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae') < 0.01):
      print("MAE has reached below 10%")
      self.model.stop_training = True
callbacks = myCallback()

"""build sequential model with 1 layer LSTM and Dense"""

model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])),
    # tf.keras.layers.LSTM(128),
    # tf.keras.layers.Dropout(rate=0.2),
    # tf.keras.layers.Dense(30, activation="relu"),
    # tf.keras.layers.Dense(1, activation="relu"),
    tf.keras.layers.Dense(1)
])

"""using learning rate as optimizer, and mae as metrics"""

# optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
# model.compile(loss=tf.keras.losses.Huber(),
#               optimizer=optimizer,
#               metrics=["mae"])

optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

"""training model with 300 epochs, and shuffle false"""

history = model.fit(trainX, trainY,
                    epochs=300,
                    batch_size=100,
                    validation_data=(testX, testY),
                    verbose=2,
                    callbacks = callbacks,
                    shuffle=False)

"""visualize the history between loss and mae of training model"""

def final_history(history):
    fig, ax = plt.subplots(1, 2, figsize=(15,5))
    ax[0].set_title('LOSS')
    ax[0].plot(history.epoch, history.history["loss"], label="Train Loss")
    ax[0].plot(history.epoch, history.history["val_loss"], label="Validation Loss")
    ax[1].set_title('MAE')
    ax[1].plot(history.epoch, history.history["mae"], label="Train Mae")
    ax[1].plot(history.epoch, history.history["val_mae"], label="Validation Mae")
    ax[0].legend()
    ax[1].legend()

final_history(history)